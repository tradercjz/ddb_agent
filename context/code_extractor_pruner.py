# file: ddb_agent/context/code_extractor_pruner.py

import json
from typing import List, Dict, Any, Tuple
from llm.llm_prompt import llm  # å‡è®¾æ‚¨ä½¿ç”¨ä¹‹å‰è®¾è®¡çš„llm.prompt
from token_counter import count_tokens # å¼•å…¥æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„tokenè®¡æ•°å™¨

class Document:
    """A simple container for source code or md doc data."""
    def __init__(self, file_path: str, source_code: str, tokens: int = -1):
        self.file_path = file_path
        self.source_code = source_code
        self.tokens = tokens if tokens != -1 else count_tokens(source_code)

class CodeExtractorPruner:
    """
    Implements the 'extract' context pruning strategy.
    It extracts relevant code snippets from large files based on conversation history.
    """
    def __init__(self, max_tokens: int, llm_model_name: str = "deepseek-default"):
        self.max_tokens = max_tokens
        self.llm_model_name = llm_model_name
        # è®¾ç½®ä¸€ä¸ªé˜ˆå€¼ï¼Œå°äºæ­¤é˜ˆå€¼çš„æ–‡ä»¶å°†è¢«å®Œæ•´ä¿ç•™ï¼Œä»¥æé«˜æ•ˆç‡
        self.full_file_threshold = int(max_tokens * 0.8)

    @llm.prompt(response_model=List[Dict[str, int]])
    def _extract_snippets_prompt(self, conversations: List[Dict[str, str]], content_with_lines: str) -> dict:
        """
        Based on the provided code file and conversation history, extract relevant code snippets.

        The code file content is provided below with line numbers.
        <CODE_FILE>
        {{ content_with_lines }}
        </CODE_FILE>

        Here is the conversation history leading to the current task.
        <CONVERSATION_HISTORY>
        {% for msg in conversations %}
        <{{ msg.role }}>: {{ msg.content }}
        {% endfor %}
        </CONVERSATION_HISTORY>

        Your Task:
        1. Analyze the last user request in the conversation history.
        2. Identify one or more important code sections in the code file that are relevant to this request.
        3. For each relevant section, determine its start and end line numbers.
        4. You can return up to 4 snippets.

        Output Requirements:
        - Return a JSON array of objects, where each object contains "start_line" and "end_line".
        - Line numbers must be integers and correspond to the numbers in the provided code file.
        - If no code sections are relevant, return an empty array [].
        - Your response MUST be a valid JSON array and nothing else.

        Example output:
        ```json
        [
            {"start_line": 10, "end_line": 25},
            {"start_line": 88, "end_line": 95}
        ]
        ```
        """
        # è¿™ä¸ªå‡½æ•°å°†ä½¿ç”¨ llm.prompt è£…é¥°å™¨ï¼Œè‡ªåŠ¨å¡«å……æ¨¡æ¿
        return {
            "model": self.llm_model_name,
            # conversations å’Œ content_with_lines ä¼šä»å‡½æ•°å‚æ•°ä¸­è·å–
        }
    
    def _merge_overlapping_snippets(self, snippets: List[Dict[str, int]]) -> List[Dict[str, int]]:
        """Merges overlapping or adjacent line number ranges."""
        if not snippets:
            return []

        # æŒ‰èµ·å§‹è¡Œæ’åº
        sorted_snippets = sorted(snippets, key=lambda x: x["start_line"])

        merged = [sorted_snippets[0]]
        for current in sorted_snippets[1:]:
            last = merged[-1]
            # å¦‚æœå½“å‰åŒºé—´çš„å¼€å§‹åœ¨å‰ä¸€ä¸ªåŒºé—´çš„ç»“æŸè¡Œ+1çš„èŒƒå›´å†…ï¼Œåˆ™åˆå¹¶
            if current["start_line"] <= last["end_line"] + 1:
                last["end_line"] = max(last["end_line"], current["end_line"])
            else:
                merged.append(current)
        return merged

    def _build_snippet_content(self, original_code: str, snippets: List[Dict[str, int]]) -> str:
        """Constructs the final content string from the extracted snippets."""
        lines = original_code.splitlines()
        content_parts = ["# Snippets from the original file:\n"]
        
        for snippet in snippets:
            start = max(0, snippet["start_line"] - 1)
            end = min(len(lines), snippet["end_line"])
            content_parts.append(f"\n# ... (lines {start + 1}-{end}) ...\n")
            content_parts.extend(lines[start:end])
        
        return "\n".join(content_parts)

    def prune(self, file_sources: List[Document], conversations: List[Dict[str, str]]) -> List[Document]:
        """
        Prunes the context by extracting relevant snippets from large files.
        """
        print("Starting 'extract' pruning strategy...")
        selected_files: List[Document] = []
        total_tokens = 0
        
        for file_source in file_sources:
            if total_tokens + file_source.tokens <= self.full_file_threshold:
                # 1. å®Œæ•´ä¿ç•™å°æ–‡ä»¶
                selected_files.append(file_source)
                total_tokens += file_source.tokens
                print(f"âœ… Kept file completely: {file_source.file_path} ({file_source.tokens} tokens)")
                continue

            if total_tokens >= self.max_tokens:
                print("Token limit reached. Stopping further processing.")
                break

            # 2. å¯¹å¤§æ–‡ä»¶è¿›è¡Œç‰‡æ®µæŠ½å–
            print(f"ğŸ” Processing large file for snippets: {file_source.file_path} ({file_source.tokens} tokens)")
            
            try:
                # ä¸ºæ–‡ä»¶å†…å®¹æ·»åŠ è¡Œå·
                lines_with_numbers = "\n".join(
                    f"{i+1} {line}" for i, line in enumerate(file_source.source_code.splitlines())
                )
                
                # è°ƒç”¨ LLM æŠ½å–ç‰‡æ®µ
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å‡è®¾å•ä¸ªæ–‡ä»¶æ·»åŠ è¡Œå·åä¸ä¼šè¶…è¿‡æ¨¡å‹çª—å£ï¼Œ
                # å¦‚æœä¼šï¼Œåˆ™éœ€è¦å¼•å…¥ `_split_content_with_sliding_window` é€»è¾‘
                raw_snippets = self._extract_snippets_prompt(
                    conversations=conversations,
                    content_with_lines=lines_with_numbers
                )
                
                if not raw_snippets:
                    print(f"  - No relevant snippets found in {file_source.file_path}.")
                    continue
                
                # åˆå¹¶é‡å ç‰‡æ®µ
                merged_snippets = self._merge_overlapping_snippets(raw_snippets)
                
                # æ„å»ºæ–°å†…å®¹å¹¶è®¡ç®—token
                new_content = self._build_snippet_content(file_source.source_code, merged_snippets)
                new_tokens = count_tokens(new_content, model_name=self.llm_model_name)
                
                if total_tokens + new_tokens <= self.max_tokens:
                    selected_files.append(Document(
                        file_path=file_source.file_path,
                        source_code=new_content,
                        tokens=new_tokens
                    ))
                    total_tokens += new_tokens
                    print(f"  - Extracted snippets from {file_source.file_path}. "
                          f"Original: {file_source.tokens} tokens -> New: {new_tokens} tokens.")
                else:
                    print(f"  - Snippets from {file_source.file_path} are too large to fit. Skipping.")
                    break # å¦‚æœæ·»åŠ ç‰‡æ®µåè¶…é™ï¼Œåˆ™åœæ­¢å¤„ç†åç»­æ–‡ä»¶

            except Exception as e:
                print(f"Error processing snippets for {file_source.file_path}: {e}")
                continue # å‡ºé”™åˆ™è·³è¿‡æ­¤æ–‡ä»¶

        print(f"Pruning complete. Final context has {len(selected_files)} files with {total_tokens} tokens.")
        return selected_files