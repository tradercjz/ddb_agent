# file: ddb_agent/agent.py (ä¹‹å‰åœ¨main.pyä¸­è™šæ„çš„ï¼Œç°åœ¨æ­£å¼å®ç°)

import json
import os
from typing import Generator, List, Dict, Any, Tuple
from agent.code_executor import CodeExecutor
from agent.coding_task_state import CodingTaskState
from agent.execution_result import ExecutionResult
from agent.prompts import debugging_planner, fix_script_from_error, generate_initial_script
from llm.llm_client import LLMResponse
from session.session_manager import SessionManager
from context.context_builder import ContextBuilder
from rag.rag_entry import DDBRAG
from llm.llm_prompt import llm # å‡è®¾llmå®ä¾‹åœ¨è¿™é‡Œ

from rich.pretty import pprint

from agent.tool_manager import ToolManager
from agent.tools.ddb_tools import GetFunctionSignatureTool, RunDolphinDBScriptTool
from agent.tools.enhanced_ddb_tools import (
    InspectDatabaseTool, ListTablesTool, DescribeTableTool, 
    ValidateScriptTool, QueryDataTool, CreateSampleDataTool, OptimizeQueryTool
)
from agent.enhanced_planner import EnhancedPlanner
from agent.enhanced_executor import EnhancedExecutor
from utils.json_parser import parse_json_string


class DDBAgent:
    """
    The main agent orchestrating all components: session, RAG, context, and LLM.
    """
    def __init__(self, project_path: str, model_name: str, max_window_size: int):
        self.project_path = project_path
        self.session_manager = SessionManager(project_path=project_path)
        self.context_builder = ContextBuilder(model_name=model_name, max_window_size=max_window_size)
        self.rag = DDBRAG(project_path=project_path)
        self.llm_model_name = model_name
        self.code_executor = CodeExecutor()
        # åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨ï¼ˆåŒ…å«å¢å¼ºå·¥å…·é›†ï¼‰
        self.tool_manager = ToolManager([
            # åŸºç¡€å·¥å…·
            RunDolphinDBScriptTool(),
            GetFunctionSignatureTool(),
            # å¢å¼ºå·¥å…·é›†
            InspectDatabaseTool(),
            ListTablesTool(),
            DescribeTableTool(),
            ValidateScriptTool(),
            QueryDataTool(),
            CreateSampleDataTool(),
            OptimizeQueryTool()
        ])
        
        # åˆå§‹åŒ–å¢å¼ºè§„åˆ’å™¨å’Œæ‰§è¡Œå™¨
        self.enhanced_planner = EnhancedPlanner(self.tool_manager, self.rag)
        self.enhanced_executor = EnhancedExecutor(self.tool_manager, self.enhanced_planner)
        self.last_successful_script: str | None = None 

        # å®šä¹‰ä¸€ä¸ªé€šç”¨çš„èŠå¤©Prompt
        @llm.prompt()
        def _default_chat_prompt(conversation_history: List[Dict[str, str]]):
            """
            You are a helpful DolphinDB assistant. Continue the conversation naturally.
            The user's latest message is the last one in the history.
            """
            # è¿™ä¸ªå‡½æ•°åªç”¨äºä¼ é€’å†å²ï¼Œæ‰€ä»¥è¿”å›ç©ºå­—å…¸
            return {"model": self.llm_model_name}
        
        self.chat_prompt_func = _default_chat_prompt

    def start_new_session(self):
        """Starts a new chat session."""
        self.session_manager.new_session()

    @llm.prompt(stream=True)
    def _streaming_chat_prompt(self, conversation_history: List[Dict[str, str]]):
        """       You are a helpful DolphinDB assistant. Continue the conversation naturally.
        The user's latest message is the last one in the history.
        """
    
    def _stream_wrapper(self, generator):
        """ä¸€ä¸ªåŒ…è£…å™¨ï¼Œç”¨äºåœ¨æµå¼è¾“å‡ºç»“æŸåä¿å­˜å†å²è®°å½•ã€‚"""
        full_content = ""
        final_meta = None
        for part in generator:
            if isinstance(part, str):
                full_content += part
                yield part # å°†æ–‡æœ¬å—ä¼ é€’å‡ºå»
            elif isinstance(part, LLMResponse):
                final_meta = part
        
        # æµç»“æŸåï¼Œä¿å­˜å®Œæ•´å¯¹è¯
        if final_meta and final_meta.success:
            self.session_manager.add_message('assistant', full_content)
            self.session_manager.save_session()
        
        # å°†æœ€åçš„å…ƒæ•°æ®ä¹Ÿä¼ é€’å‡ºå»ï¼Œä»¥ä¾¿ä¸Šå±‚æ£€æŸ¥é”™è¯¯
        #if final_meta:
        #    yield final_meta

    def run_task(self, user_input: str, task_type: str = 'chat', stream: bool = False) :
        """
        Handles a user request by orchestrating RAG, context building, and LLM interaction.
        """
        # 1. æ›´æ–°ä¼šè¯å†å²
        self.session_manager.add_message('user', user_input)
        full_conversation_history = self.session_manager.get_history()

        # 2. ä½¿ç”¨ RAG æ£€ç´¢ç›¸å…³æ–‡ä»¶ä¸Šä¸‹æ–‡
        # æˆ‘ä»¬ç”¨æœ€æ–°çš„ç”¨æˆ·è¾“å…¥å»æ£€ç´¢
        relevant_files = self.rag.retrieve(user_input, top_k=5)

        # 3. å‡†å¤‡æ„å»ºä¸Šä¸‹æ–‡æ‰€éœ€çš„æ‰€æœ‰ææ–™
        system_prompt = "You are a world-class DolphinDB expert. Answer the user's query based on the provided context. If file context is provided, prioritize it. Be concise, accurate, and provide code examples where appropriate."
        
        # 4. ä½¿ç”¨ ContextBuilder æ„å»ºæœ€ç»ˆçš„ã€ç»è¿‡å‰ªæçš„ä¸Šä¸‹æ–‡
        # æ³¨æ„ï¼šæˆ‘ä»¬å°†å®Œæ•´çš„å†å²å’Œæ£€ç´¢åˆ°çš„æ–‡ä»¶éƒ½ä¼ ç»™å®ƒ
        final_messages = self.context_builder.build(
            system_prompt=system_prompt,
            conversations=full_conversation_history,
            file_sources=relevant_files,
            task_type=task_type,
            file_pruning_strategy='extract'
        )
        
        # 5. è°ƒç”¨ LLM
        # è¿™é‡Œæˆ‘ä»¬ä¸å†ä½¿ç”¨ç®€å•çš„ chat_oaiï¼Œè€Œæ˜¯åˆ©ç”¨æˆ‘ä»¬ä¹‹å‰è®¾è®¡çš„ llm.prompt æ¡†æ¶
        # æ¥è°ƒç”¨ä¸€ä¸ªå¸¦æœ‰å®Œæ•´ã€å‰ªæåå†å²çš„ prompt å‡½æ•°ã€‚
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸å†éœ€è¦ä¸€ä¸ªå¤æ‚çš„æ¨¡æ¿ï¼Œå› ä¸ºæ‰€æœ‰ä¸Šä¸‹æ–‡éƒ½å·²åœ¨ message åˆ—è¡¨ä¸­ã€‚
        # æˆ‘ä»¬åªéœ€ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥è§¦å‘è°ƒç”¨ã€‚

        if stream:
            response_generator = self._streaming_chat_prompt(
                conversation_history=final_messages
            )
            return self._stream_wrapper(response_generator)
        else:
            assistant_response = self.chat_prompt_func(
                conversation_history=final_messages
            )

        # 6. æ›´æ–°ä¼šè¯å¹¶ä¿å­˜
        self.session_manager.add_message('assistant', assistant_response)
        self.session_manager.save_session()

        return assistant_response
    
    def run_coding_task(self, user_input: str):
        """
        Orchestrates the iterative process of generating, executing, and fixing code.
        """
        print(f"--- Starting new coding task for: '{user_input}' ---")

        # 1. åˆå§‹ RAG
        print("Step 1: Retrieving context with RAG...")
        initial_context = self.rag.retrieve(user_input, top_k=5)
        # å°† Document åˆ—è¡¨è½¬æ¢ä¸ºå•ä¸ªå­—ç¬¦ä¸²
        context_str = "\n---\n".join(
            f"File: {doc.file_path}\n\n{doc.source_code}" for doc in initial_context
        )

        # 2. åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        state = CodingTaskState(
            original_query=user_input,
            rag_context=context_str
        )

        # 3. ç”Ÿæˆç¬¬ä¸€ç‰ˆè„šæœ¬
        print("Step 2: Generating initial script...")
        state.current_code = generate_initial_script(
            user_query=state.original_query,
            rag_context=state.rag_context
        )
        print(f"Initial script generated:\n{state.current_code}")

        # 4. è¿›å…¥æ ¸å¿ƒçš„ "æ‰§è¡Œ-ä¿®æ­£" å¾ªç¯
        while not state.has_reached_max_attempts:
            print(f"\n--- Attempt {state.refinement_attempts + 1}/{state.max_attempts} ---")
            
            # æ‰§è¡Œä»£ç 
            print("Executing script...")
            exec_result = self.code_executor.run(state.current_code)
            state.add_execution_result(exec_result)

            # åˆ†æç»“æœ
            if exec_result.success:
                print("âœ… Task Succeeded!")
                print("source code:",state.current_code)  # è¾“å‡ºæœ€ç»ˆä»£ç 
                print("Final Result Data:")
                print(exec_result.data)
                # ä»»åŠ¡æˆåŠŸï¼Œé€€å‡ºå¾ªç¯
                return exec_result
            
            # å¦‚æœå¤±è´¥ï¼Œè¿›è¡Œä¿®æ­£
            print(f"Script failed. Error: {exec_result.error_message}")
            print("Attempting to self-correct...")
            
            last_error = state.get_last_error()
            
            # (å¯é€‰) é’ˆå¯¹é”™è¯¯è¿›è¡Œ RAG
            # error_context = self.rag.retrieve(last_error, top_k=2)
            # combined_context = state.rag_context + "\n---\n" + error_context_str
            
            # è°ƒç”¨ä¿®æ­£ prompt
            state.current_code = fix_script_from_error(
                original_query=state.original_query,
                failed_code=state.current_code,
                error_message=last_error,
                rag_context=state.rag_context # ä½¿ç”¨æ›´æ–°åçš„ä¸Šä¸‹æ–‡
            )
            print(f"Generated new corrected script:\n{state.current_code}")
            
            state.refinement_attempts += 1
        
        # å¦‚æœå¾ªç¯ç»“æŸä»æœªæˆåŠŸ
        print("âŒ Task Failed after maximum attempts.")
        return state.execution_history[-1] # è¿”å›æœ€åä¸€æ¬¡çš„å¤±è´¥ç»“æœ
    
    def run_coding_task_with_planner(self, user_input: str) -> Generator[Dict[str, Any], None, None]:
        """
        Orchestrates the plan-and-execute loop for a coding task, yielding state updates.
        """
        yield {"type": "status", "message": "Starting new PLAN-and-EXECUTE coding task..."}


        # 2. ç”Ÿæˆåˆå§‹è®¡åˆ’
        yield {"type": "status", "message": "Generating initial plan..."}
        # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–ï¼Œç›´æ¥ç”Ÿæˆä¸€ä¸ªåŒ…å«run_dolphindb_scriptçš„è®¡åˆ’
        # å®é™…ä¸­å¯èƒ½éœ€è¦ä¸€ä¸ªPlanneræ¥ç”Ÿæˆ
        try:
            initial_script = generate_initial_script(user_query=user_input, rag_context="...") # å‡è®¾æœ‰rag_context
            plan = [
                {
                    "step": 1, 
                    "thought": "I will start by generating a script to address the user's request and then execute it.", 
                    "action": "run_dolphindb_script", 
                    "args": {"script": initial_script}
                }
            ]
            yield {"type": "plan", "plan": plan, "message": "Initial plan generated."}
        except Exception as e:
            yield {"type": "error", "message": f"Failed to generate initial script: {e}"}
            return

        # 3. æ‰§è¡Œè®¡åˆ’å¾ªç¯
        step_index = 0
        execution_context = {}

        while step_index < len(plan):
            current_step = plan[step_index]
            action = current_step["action"]
            args = current_step["args"]
            thought = current_step["thought"]
            
            # Yield å½“å‰æ­¥éª¤çš„æ€è€ƒè¿‡ç¨‹
            yield {"type": "step_start", "step": step_index + 1, "thought": thought, "action": action, "args": args}

            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_result = self.tool_manager.call_tool(action, args)

            is_success = True
            if isinstance(tool_result, ExecutionResult):
                observation_str = str(tool_result.data) if tool_result.success else f"Execution failed. Error:\n{tool_result.error_message}"
                is_success = tool_result.success
            else: # It's a string from another tool like get_function_signature
                observation_str = str(tool_result)

            yield {"type": "step_result", "step": step_index + 1, "observation": observation_str}

            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯åŠ¨è°ƒè¯•å­æµç¨‹
            if action == "run_dolphindb_script" and  not is_success:
                yield {"type": "status", "message": "Execution failed. Entering debugging sub-task..."}
                
                failed_code = args["script"]
                error_message = observation_str.split("Error:\n", 1)[1]
                tool_defs_str = json.dumps(self.tool_manager.get_tool_definitions(), indent=2)

                try:
                    # è°ƒç”¨è°ƒè¯•Planner
                    new_plan_str = debugging_planner(
                        original_query=user_input,
                        failed_code=failed_code,
                        error_message=error_message,
                        tool_definitions=tool_defs_str
                    )
                    new_plan = parse_json_string(new_plan_str)

                    # Yield æ–°çš„è°ƒè¯•è®¡åˆ’
                    yield {"type": "plan", "plan": new_plan, "message": "Generated a new debugging plan."}
                    
                    plan = new_plan
                    step_index = 0
                    continue # é‡ç½®å¾ªç¯ï¼Œä»æ–°è®¡åˆ’çš„ç¬¬ä¸€æ­¥å¼€å§‹
                except Exception as e:
                    yield {"type": "error", "message": f"Failed to generate debugging plan: {e}"}
                    return

            execution_context[f"step_{step_index + 1}_result"] = tool_result
            step_index += 1
        
        final_result_obj = execution_context.get(f"step_{len(plan)}_result")

        if final_result_obj and isinstance(final_result_obj, ExecutionResult) and final_result_obj.success:
            self.last_successful_script = final_result_obj.executed_script 
        else:
            # If the task fails or doesn't end with a script, clear the last script
            self.last_successful_script = None 

        yield {"type": "final_result", "result_object": final_result_obj}
    
    def run_enhanced_coding_task(self, user_input: str) -> Generator[Dict[str, Any], None, None]:
        """
        ä½¿ç”¨å¢å¼ºçš„plan/actæ¨¡å¼æ‰§è¡Œç¼–ç ä»»åŠ¡
        """
        yield {"type": "status", "message": "ğŸš€ Starting enhanced coding task..."}
        
        try:
            # ä½¿ç”¨å¢å¼ºæ‰§è¡Œå™¨æ‰§è¡Œä»»åŠ¡
            for update in self.enhanced_executor.execute_task(user_input):
                # ä¿å­˜æœ€åæˆåŠŸçš„è„šæœ¬
                if (update.get("type") == "final_result" and 
                    update.get("result_object") and 
                    isinstance(update["result_object"], ExecutionResult) and 
                    update["result_object"].success):
                    self.last_successful_script = update["result_object"].executed_script
                
                yield update
                
        except Exception as e:
            yield {
                "type": "error", 
                "message": f"Enhanced coding task failed: {str(e)}"
            }

    def save_last_script(self, file_path: str) -> Tuple[bool, str]:
        """
        Saves the last successfully executed script to a file.
        
        Returns:
            A tuple of (success: bool, message: str).
        """
        if not self.last_successful_script:
            return False, "No successful script is available to save. Please run a /code task first."
        
        try:
            # Create directories if they don't exist
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(self.last_successful_script)
            
            return True, f"Script successfully saved to: {file_path}"
        except Exception as e:
            return False, f"Error saving file: {e}"